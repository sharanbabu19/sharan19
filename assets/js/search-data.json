{
  
    
        "post0": {
            "title": "Tensorflow Tutorials and Links",
            "content": "# Early stopping # Tensorboard # Saving checkpoint # Saving Model # Keras Tuner . Quickstart for Experts: Link . Convert to One hot encoded categorical datatype . from keras.utils import to_categorical y_train_b = to_categorical(y_train) . Plotting sample Images from the Dataset . Link to colab notebook with MNIST DNN model building and visualization . import matplotlib.pyplot as plt plt.figure(figsize=(10,10)) for i in range(25): plt.subplot(5,5, i+1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(X_train[i], cmap= plt.cm.binary) # u can map the index to class name using class_names[train_labels[i]] if needed plt.xlabel(class_names[y_train[i]]) plt.show() . . Evaluating accuracy on test data . test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) . Predictions on validation_data or batched data . predictions = probability_model.predict(test_images) . Prediction on a Single Image . img = test_images[1] img = (np.expand_dims(img,0)) predictions_single = model.predict(img) # Get prediction index position prediction_index = np.argmax(predictions_single[0]) . . TFDS and TF HUB for NLP Example . Link to the tutorial of the same. . . EARLY STOPPING . Use an EarlyStopping callback that tests a training condition for every epoch. If a set amount of epochs elapses without showing improvement, then automatically stop the training. . model = build_model() # The patience parameter is the amount of epochs to check for improvement # tf.keras.callbacks.EarlyStopping is also an option early_stop = keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, patience=10) early_history = model.fit(train_data, train_labels, epochs=EPOCHS, validation_split = 0.2, verbose=0, callbacks=[early_stop]) . . Using Tensorboard for Model metric Visualizations . Tensorboard usage example in google colab . tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir) history = model.fit(train_data, train_labels, epochs=EPOCHS, validation_split = 0.2, verbose=0, callbacks=[tensorboard_callback]) # Load the TensorBoard notebook extension %load_ext tensorboard # Open an embedded TensorBoard viewer # sizes is the name of the model %tensorboard --logdir logs/fit #%tensorboard --logdir {logdir}/sizes # regularizers . Example of L2 Regularization for preventing overfitting . Link to notebook that shows code example . l2_model = tf.keras.Sequential([ layers.Dense(512, activation=&#39;relu&#39;, kernel_regularizer=regularizers.l2(0.001), input_shape=(FEATURES,)), layers.Dense(512, activation=&#39;relu&#39;, kernel_regularizer=regularizers.l2(0.001)), layers.Dense(1)]) . . Saving checkpoints . Link to notebook for saving models . !pip install -q pyyaml h5py # Import necessary libraries import os import tensorflow as tf from tensorflow import keras # ....... model = create_model() # Create checkpoint path - weights will be saved in a new training_1 folder checkpoint_path = &quot;training_1/cp.ckpt&quot; checkpoint_dir = os.path.dirname(checkpoint_path) # Create a callback that saves the model&#39;s weights # If save_weights_only parameter is false then entire model is downloaded # (assests, variables and .pb weights file) cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1) # Train the model with the new callback model.fit(train_images, train_labels, epochs=10, validation_data=(test_images,test_labels), callbacks=[cp_callback]) # This creates a single collection of TensorFlow checkpoint files that are # updated at the end of each epoch # ! ls {checkpoint_dir} -- To check the saved files in the directory from colab . Create a new, untrained model. When restoring a model from weights only, you must have a model with the same architecture as the original model. Since it&#39;s the same model architecture, you can share weights despite that it&#39;s a different instance of the model. . model = create_model() model.load_weights(checkpoint_path) # Now the model is restored . Saving checkpoint every 5 epochs and loading the latest checkpoint&lt;/h5&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; checkpoint_path = &quot;training_2/cp-{epoch:04d}.ckpt&quot; checkpoint_dir = os.path.dirname(checkpoint_path) # Create a callback that saves the model&#39;s weights every 5 epochs cp_callback = tf.keras.callbacks.ModelCheckpoint( filepath=checkpoint_path, verbose=1, save_weights_only=True, period=5) # Create a new model instance model = create_model() # Save the weights using the `checkpoint_path` format model.save_weights(checkpoint_path.format(epoch=0)) # Train the model with the new callback model.fit(train_images, train_labels, epochs=50, callbacks=[cp_callback], validation_data=(test_images,test_labels), verbose=0) # Steps to Load this model checkpoints # 1) Choose the latest checkpoint latest = tf.train.latest_checkpoint(checkpoint_dir) # 2) Create a new model instance model.load_eights(latest) # Model restored! . Manually save weights&lt;/h5&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; model.save_weights(&#39;./checkpoints/my_checkpoint&#39;) # Create a new model instance model = create_model() # Restore the weights model.load_weights(&#39;./checkpoints/my_checkpoint&#39;) # Evaluate the model loss,acc = model.evaluate(test_images, test_labels, verbose=2) print(&quot;Restored model, accuracy: {:5.2f}%&quot;.format(100*acc)) . Save the entire model&lt;/h5&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; !mkdir -p saved_model model.save(&#39;saved_model/my_model&#39;) !ls saved_model/my_model # O/p: assets saved_model.pb variables # Restoring model new_model = tf.keras.models.load_model(&#39;saved_model/my_model&#39;) . Save in HDF5 format&lt;/h5&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; model.save(&#39;my_model.h5&#39;) # Restore the model new_model = tf.keras.models.load_model(&#39;my_model.h5&#39;) . . Keras Tuner: . Link to notebook with code for auto tuner . Hyper Parameter Tuning . !pip install -q -U keras-tuner import kerastuner as kt # Ex: def model_builder(hp): model = keras.Sequential() model.add(keras.layers.Flatten(input_shape=(28, 28))) # Tune the number of units in the first Dense layer # Choose an optimal value between 32-512 hp_units = hp.Int(&#39;units&#39;, min_value = 32, max_value = 512, step = 32) model.add(keras.layers.Dense(units = hp_units, activation = &#39;relu&#39;)) model.add(keras.layers.Dense(10)) # Tune the learning rate for the optimizer # Choose an optimal value from 0.01, 0.001, or 0.0001 hp_learning_rate = hp.Choice(&#39;learning_rate&#39;, values = [1e-2, 1e-3, 1e-4]) model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate), loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True), metrics = [&#39;accuracy&#39;]) return model # Instantiate the tuner and perform hypertuning tuner = kt.Hyperband(model_builder, objective = &#39;val_accuracy&#39;, max_epochs = 10, factor = 3, directory = &#39;my_dir&#39;, project_name = &#39;intro_to_kt&#39;) # To ensure training output is cleared after each trial thereby keeping # notebook clean and not cluttered class ClearTrainingOutput(tf.keras.callbacks.Callback): def on_train_end(*args, **kwargs): IPython.display.clear_output(wait = True) tuner.search(img_train, label_train, epochs = 10, validation_data = (img_test, label_test), callbacks = [ClearTrainingOutput()]) # Get the optimal hyperparameters best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0] print(f&quot;{best_hps.get(&#39;units&#39;)}&quot;) # Build the model with the optimal hyperparameters and train it on the data model = tuner.hypermodel.build(best_hps) model.fit(img_train, label_train, epochs = 10, validation_data = (img_test, label_test)) . . Pre-processing if Dataset is in numpy format . Link to tutorial . . Pre-Processing Image Data . Link to tutorial . import pathlib dataset_url = &quot;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&quot; # Imp: data_dir = tf.keras.utils.get_file(origin=dataset_url, fname=&#39;flower_photos&#39;, untar=True) data_dir = pathlib.Path(data_dir) . image_count = len(list(data_dir.glob(&#39;*/*.jpg&#39;))) print(image_count) . roses = list(data_dir.glob(&#39;roses/*&#39;)) PIL.Image.open(str(roses[0])) . batch_size = 32 img_height = 180 img_width = 180 train_ds = tf.keras.preprocessing.image_dataset_from_directory( data_dir, validation_split=0.2, subset=&quot;training&quot;, seed=123, image_size=(img_height, img_width), batch_size=batch_size) val_ds = tf.keras.preprocessing.image_dataset_from_directory( data_dir, validation_split=0.2, subset=&quot;validation&quot;, seed=123, image_size=(img_height, img_width), batch_size=batch_size) class_names = train_ds.class_names print(class_names) # After this code is same as usual. . . Pre-processing Text Data . Tutorial Link . . Unicode pre-processing . Tutorial . . Custom Layer Building . Tutorial Tutorial 2 . Custom Training . Tutorial . . Image Classification with augmentation (as layers) . Link . !pip install -q tf-nightly import matplotlib.pyplot as plt import numpy as np import os import PIL import tensorflow as tf # Download and explore the dataset import pathlib dataset_url = &quot;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&quot; data_dir = tf.keras.utils.get_file(&#39;flower_photos&#39;, origin=dataset_url, untar=True) data_dir = pathlib.Path(data_dir) # Make a list of the returned generator function which gets back only.jpg images image_count = len(list(data_dir.glob(&#39;*/*.jpg&#39;))) print(image_count) # List of all images of roses. Make it str to read it. roses = list(data_dir.glob(&#39;roses/*&#39;)) PIL.Image.open(str(roses[0])) # Loading using keras.preprocessing batch_size = 32 img_height = 180 img_width = 180 #.image_dataset_from_directory currently only in tf-nightly train_ds = tf.keras.preprocessing.image_dataset_from_directory( data_dir, validation_split=0.2, subset=&quot;training&quot;, seed=123, image_size=(img_height, img_width), batch_size=batch_size) val_ds = tf.keras.preprocessing.image_dataset_from_directory( data_dir, validation_split=0.2, subset=&quot;validation&quot;, seed=123, image_size=(img_height, img_width), batch_size=batch_size) class_names = train_ds.class_names print(class_names) AUTOTUNE = tf.data.experimental.AUTOTUNE train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE) val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE) normalization_layer = layers.experimental.preprocessing.Rescaling(1./255) normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y)) image_batch, labels_batch = next(iter(normalized_ds)) # create model num_classes = 5 model = Sequential([ layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)), layers.Conv2D(16, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), layers.MaxPooling2D(), layers.Conv2D(32, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), layers.MaxPooling2D(), layers.Conv2D(64, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), layers.MaxPooling2D(), layers.Flatten(), layers.Dense(128, activation=&#39;relu&#39;), layers.Dense(num_classes) ]) # compile model model.compile(optimizer=&#39;adam&#39;, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[&#39;accuracy&#39;]) # train model epochs=10 history = model.fit( train_ds, validation_data=val_ds, epochs=epochs ) # Data augmentation data_augmentation = keras.Sequential( [ layers.experimental.preprocessing.RandomFlip(&quot;horizontal&quot;, input_shape=(img_height, img_width, 3)), layers.experimental.preprocessing.RandomRotation(0.1), layers.experimental.preprocessing.RandomZoom(0.1), ] ) # Visualizing augmentation plt.figure(figsize=(10, 10)) for images, _ in train_ds.take(1): for i in range(9): augmented_images = data_augmentation(images) ax = plt.subplot(3, 3, i + 1) plt.imshow(augmented_images[0].numpy().astype(&quot;uint8&quot;)) plt.axis(&quot;off&quot;) # adding augmentation and dropout layers model = Sequential([ data_augmentation, layers.experimental.preprocessing.Rescaling(1./255), layers.Conv2D(16, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), layers.MaxPooling2D(), layers.Conv2D(32, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), layers.MaxPooling2D(), layers.Conv2D(64, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), layers.MaxPooling2D(), layers.Dropout(0.2), layers.Flatten(), layers.Dense(128, activation=&#39;relu&#39;), layers.Dense(num_classes) ]) # Compile,train and fit...... # Predict on new data sunflower_url = &quot;https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg&quot; sunflower_path = tf.keras.utils.get_file(&#39;Red_sunflower&#39;, origin=sunflower_url) img = keras.preprocessing.image.load_img( sunflower_path, target_size=(img_height, img_width) ) img_array = keras.preprocessing.image.img_to_array(img) img_array = tf.expand_dims(img_array, 0) # Create a batch predictions = model.predict(img_array) score = tf.nn.softmax(predictions[0]) print( &quot;This image most likely belongs to {} with a {:.2f} percent confidence.&quot; .format(class_names[np.argmax(score)], 100 * np.max(score)) ) . . Transfer Learning With TF HUB . Link . !pip install -q -U tf-hub-nightly !pip install -q tfds-nightly import tensorflow_hub as hub # ImageNet classifier classifier_url =&quot;https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/2&quot; IMAGE_SHAPE = (224, 224) # Make it a Keras layer classifier = tf.keras.Sequential([ hub.KerasLayer(classifier_url, input_shape=IMAGE_SHAPE+(3,)) ]) # Labels- Now u can directly use this to predict on classes this was built # to predict or use it for ur own dataset labels_path = tf.keras.utils.get_file(&#39;ImageNetLabels.txt&#39;,&#39;https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt&#39;) imagenet_labels = np.array(open(labels_path).read().splitlines()) # Transfer learning for custom dataset # Download the headless model feature_extractor_url = &quot;https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2&quot; feature_extractor_layer = hub.KerasLayer(feature_extractor_url, input_shape=(224,224,3)) feature_extractor_layer.trainable = False # Attach a classification head model = tf.keras.Sequential([ feature_extractor_layer, layers.Dense(image_data.num_classes) ]) model.summary() # Now compile and fit..... . . Transfer Learning with Pre-Trained CNN . preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1) IMG_SIZE = (160, 160) # Create the base model from the pre-trained model MobileNet V2 IMG_SHAPE = IMG_SIZE + (3,) base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights=&#39;imagenet&#39;) # This feature extractor converts each 160x160x3 image into a 5x5x1280 block of features image_batch, label_batch = next(iter(train_dataset)) feature_batch = base_model(image_batch) print(feature_batch.shape) # DATA AUGMENTATION LAYERS data_augmentation = tf.keras.Sequential([ tf.keras.layers.experimental.preprocessing.RandomFlip(&#39;horizontal&#39;), tf.keras.layers.experimental.preprocessing.RandomRotation(0.2), ]) # Freeze the convolutional base base_model.trainable = False global_average_layer = tf.keras.layers.GlobalAveragePooling2D() feature_batch_average = global_average_layer(feature_batch) prediction_layer = tf.keras.layers.Dense(1) prediction_batch = prediction_layer(feature_batch_average) inputs = tf.keras.Input(shape=(160, 160, 3)) x = data_augmentation(inputs) x = preprocess_input(x) x = base_model(x, training=False) x = global_average_layer(x) x = tf.keras.layers.Dropout(0.2)(x) outputs = prediction_layer(x) model = tf.keras.Model(inputs, outputs) # compile model base_learning_rate = 0.0001 model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=[&#39;accuracy&#39;]) # train model history = model.fit(train_dataset, epochs=initial_epochs, validation_data=validation_dataset) # Fine Tuning base_model.trainable = True # Fine-tune from this layer onwards fine_tune_at = 100 # Freeze all the layers before the `fine_tune_at` layer for layer in base_model.layers[:fine_tune_at]: layer.trainable = False # Now u can compile and fit again model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate/10), metrics=[&#39;accuracy&#39;]) fine_tune_epochs = 10 total_epochs = initial_epochs + fine_tune_epochs history_fine = model.fit(train_dataset, epochs=total_epochs, initial_epoch=history.epoch[-1], validation_data=validation_dataset) . . Object Detection . Link to Colab Notebook . . Classification on Imbalanced Data - DL . Link to tutorial . . Time Series Forcasting . Link . . &lt;/div&gt; .",
            "url": "https://sharanbabu19.github.io/sharan19/tensorflow/deep%20learning/2020/10/15/Tensorflow-Tutorials.html",
            "relUrl": "/tensorflow/deep%20learning/2020/10/15/Tensorflow-Tutorials.html",
            "date": " ‚Ä¢ Oct 15, 2020"
        }
        
    
  
    
  
    
        ,"post2": {
            "title": "Html (5q9) Bts",
            "content": "Scripting in HTML . 1) Basic Program . &lt;html&gt; . &lt;head&gt; . &lt;title&gt; My first Js program&lt;/title&gt;&lt;/head&gt; . &lt;body&gt; . &lt;center&gt; . &lt;script type=‚Äùtext/javascript‚Äù&gt; . document.write(‚ÄúWelcome to the browser using Js‚Äù;) . &lt;/center&gt; . &lt;/body&gt; . &lt;/html&gt; . 2) Addition . &lt;html&gt;&lt;head&gt;&lt;title&gt; Add with Js&lt;/title&gt;&lt;/head&gt; . &lt;body&gt; . &lt;center&gt; . &lt;script type=‚Äùtext/javascript‚Äù&gt; . var a,b,c; var string; . a=2; b=3; c=a+b; . string = ‚Äúthe result=‚Äù; . document.write(‚ÄúAddition result‚Äù+‚Äù&lt;br&gt;‚Äù); . document.write(c); . &lt;/script&gt; . 3) Subtraction . &lt;html&gt;&lt;head&gt;&lt;title&gt; Subtract with Js&lt;/title&gt;&lt;/head&gt; . &lt;body&gt; . &lt;center&gt; . &lt;script type=‚Äùtext/javascript‚Äù&gt; . var a,b,c; var string; . a=3; b=3; c=a-b; . string = ‚Äúthe result=‚Äù; . document.write(‚ÄúSubtraction result‚Äù+‚Äù&lt;br&gt;‚Äù); . document.write(c); . &lt;/script&gt; . 4) Largest Number . &lt;html&gt;&lt;head&gt;&lt;title&gt; Finding Largest Number&lt;/title&gt;&lt;/head&gt; . &lt;body&gt; . &lt;center&gt; . &lt;script type=‚Äùtext/javascript‚Äù&gt; . var a,b,c; . a=10; b=20;c=30; . if(a&gt;b) . {if(a&gt;c){document.write(‚Äú&lt;h1&gt; a is the largest number&lt;/h1&gt;; . else . {document.write(‚Äú&lt;h2&gt; c is the largest number &lt;/h2&gt;‚Äù);}} . else . {if (b&gt;0) . {document.write(‚Äú&lt;h2&gt; b is the largest number &lt;/h2&gt;‚Äù);} . Else . {document.write(‚Äú&lt;h2&gt; c is the largest number &lt;/h2&gt;‚Äù);}} . &lt;/script&gt; . &lt;/body&gt; . &lt;/html&gt; . 5) Table of Squared numbers . &lt;html&gt; . &lt;head&gt;&lt;title&gt; Squaring Numbers&lt;/title&gt;&lt;/head&gt; . &lt;body&gt; . &lt;table border=1 align=‚Äùcenter‚Äù&gt; . &lt;th&gt; Number &lt;/th&gt; &lt;th&gt; Square &lt;/th&gt; . &lt;script type=‚Äùtext/javascript‚Äù&gt; . var i=1; . while(i&lt;=10) . {document.write(‚Äú&lt;tr&gt;&lt;td&gt;‚Äù +i+‚Äù&lt;/td&gt;&lt;td&gt;‚Äù + (i*i) + ‚Äú&lt;/td&gt; &lt;/tr&gt;‚Äù); . i++;} . &lt;/script&gt; . &lt;/body&gt; . &lt;/html&gt; . 6) Table of Squared numbers with For Loop . &lt;html&gt; . &lt;head&gt;&lt;title&gt; Squaring Numbers with For Loop&lt;/title&gt;&lt;/head&gt; . &lt;body&gt; . &lt;table border=1 align=‚Äùcenter‚Äù&gt; . &lt;th&gt; Number &lt;/th&gt; &lt;th&gt; Square &lt;/th&gt; . &lt;script type=‚Äùtext/javascript‚Äù&gt; . var i; . for(i=1; i&lt;=10; i++) . {document.write(‚Äú&lt;tr&gt;&lt;td&gt;‚Äù +i+‚Äù&lt;/td&gt;&lt;td&gt;‚Äù + (i*i) + ‚Äú&lt;/td&gt; &lt;/tr&gt;‚Äù);} . &lt;/script&gt; . &lt;/body&gt; . &lt;/html&gt; . 7) Switch Case Js scripting in HTML . &lt;html&gt; . &lt;head&gt;&lt;title&gt; Using Switch in Js &lt;/title&gt;&lt;/head&gt; . &lt;body&gt; . &lt;script type= ‚Äútext/javascript‚Äù&gt; . D = new Data(); . Ch = d.getDays(); . Switch(ch) . { . case 1: document.write(‚ÄúSunday‚Äù); . break; . case 2: document.write(‚ÄúMonday‚Äù); . break; . case 3: document.write(‚ÄúTuesday‚Äù); . break; . case 4: document.write(‚ÄúWednesday‚Äù); . break; . case 5: document.write(‚ÄúThursday‚Äù); . break; . case 6: document.write(‚ÄúFriday‚Äù); . break; . case 7: document.write(‚ÄúSaturday‚Äù); . break;} . &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; . 8) Array . &lt;html&gt; . &lt;head&gt;&lt;title&gt; arrays &lt;/title&gt;&lt;/head&gt; . &lt;body&gt; . &lt;script type= ‚Äútext/javascript‚Äù&gt; . A = new Array(5); . for(i=0; i&lt;5; i++) . { A[i] = i; . document.write(‚Äòanother way of initialization‚Äô); . b = new Array(22, 43, 56, 74, 84); . for(i=0;i&lt;5;i++) . { . document.write(b[i] + ‚Äú&lt;br&gt;‚Äù); . } . document.write(‚Äúyet another way of initialization‚Äù); . var c= new Array(8, 16, 24, 32, 40); . for(i&lt;0;i&lt;5;i++) { . document.write(c[i]); } . document.write(‚Äúfinish‚Äù); . &lt;/script&gt; . &lt;/body&gt; . &lt;/html&gt; .",
            "url": "https://sharanbabu19.github.io/sharan19/2020/10/04/html-(5Q9)-bts.html",
            "relUrl": "/2020/10/04/html-(5Q9)-bts.html",
            "date": " ‚Ä¢ Oct 4, 2020"
        }
        
    
  
    
  
    
  
    
        ,"post5": {
            "title": "Flattening Nested Lists In Python",
            "content": "list_of_lists = [[1], [2, 3], [4, 5, 6]] sum(list_of_lists, []) ==&gt; [1, 2, 3, 4, 5, 6] .",
            "url": "https://sharanbabu19.github.io/sharan19/2020/09/24/Flattening-nested-lists-in-Python.html",
            "relUrl": "/2020/09/24/Flattening-nested-lists-in-Python.html",
            "date": " ‚Ä¢ Sep 24, 2020"
        }
        
    
  
    
  
    
        ,"post7": {
            "title": "SweetViz",
            "content": "## Package is a collection of modules. ## Library is a collection of Packages. . Hi there. Today, we are going to see how to use SweetViz library in Python which will enable us to perform powerful Exploratory Data Analysis(EDA) on your dataset. So,let&#39;s get started. . First, you will have to pip install this package as it is not an in-built Python package. You can do so from the command prompt or using !pip install sweetviz from jupyter notebook environment. . I will be using USA Housing data in this example. . !pip install sweetviz . Collecting sweetviz Downloading https://files.pythonhosted.org/packages/8f/bd/f4454adfe1d3bbd04892d6172348ca215fa62d59fb09c1ac6b8a233341d3/sweetviz-1.0a7-py3-none-any.whl (323kB) Requirement already satisfied: scipy&gt;=1.3.2 in c: users sharan babu anaconda3 lib site-packages (from sweetviz) (1.4.1) Collecting tqdm&gt;=4.43.0 (from sweetviz) Downloading https://files.pythonhosted.org/packages/f3/76/4697ce203a3d42b2ead61127b35e5fcc26bba9a35c03b32a2bd342a4c869/tqdm-4.46.1-py2.py3-none-any.whl (63kB) Collecting pandas!=1.0.0,!=1.0.1,!=1.0.2,&gt;=0.25.3 (from sweetviz) Downloading https://files.pythonhosted.org/packages/1d/eb/b4f68f54ad287d583c9c3b3c77f865615f832f092810f20d2b44498cd06c/pandas-1.0.4-cp37-cp37m-win_amd64.whl (8.7MB) Collecting matplotlib&gt;=3.1.3 (from sweetviz) Downloading https://files.pythonhosted.org/packages/b4/4d/8a2c06cb69935bb762738a8b9d5f8ce2a66be5a1410787839b71e146f000/matplotlib-3.2.1-cp37-cp37m-win_amd64.whl (9.2MB) Collecting importlib-resources&gt;=1.2.0 (from sweetviz) Downloading https://files.pythonhosted.org/packages/b6/03/1865fdd49ec9a938f9f84b255d3d37863df9fbd18b48c1c3f761040cbf13/importlib_resources-2.0.0-py2.py3-none-any.whl Collecting jinja2&gt;=2.11.1 (from sweetviz) Downloading https://files.pythonhosted.org/packages/30/9e/f663a2aa66a09d838042ae1a2c5659828bb9b41ea3a6efa20a20fd92b121/Jinja2-2.11.2-py2.py3-none-any.whl (125kB) Requirement already satisfied: numpy&gt;=1.16.0 in c: users sharan babu anaconda3 lib site-packages (from sweetviz) (1.16.4) Requirement already satisfied: pytz&gt;=2017.2 in c: users sharan babu anaconda3 lib site-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,&gt;=0.25.3-&gt;sweetviz) (2019.1) Requirement already satisfied: python-dateutil&gt;=2.6.1 in c: users sharan babu anaconda3 lib site-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,&gt;=0.25.3-&gt;sweetviz) (2.8.0) Requirement already satisfied: kiwisolver&gt;=1.0.1 in c: users sharan babu anaconda3 lib site-packages (from matplotlib&gt;=3.1.3-&gt;sweetviz) (1.1.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in c: users sharan babu anaconda3 lib site-packages (from matplotlib&gt;=3.1.3-&gt;sweetviz) (2.4.0) Requirement already satisfied: cycler&gt;=0.10 in c: users sharan babu anaconda3 lib site-packages (from matplotlib&gt;=3.1.3-&gt;sweetviz) (0.10.0) Requirement already satisfied: zipp&gt;=0.4; python_version &lt; &#34;3.8&#34; in c: users sharan babu anaconda3 lib site-packages (from importlib-resources&gt;=1.2.0-&gt;sweetviz) (0.5.1) Requirement already satisfied: importlib-metadata; python_version &lt; &#34;3.8&#34; in c: users sharan babu anaconda3 lib site-packages (from importlib-resources&gt;=1.2.0-&gt;sweetviz) (0.17) Requirement already satisfied: MarkupSafe&gt;=0.23 in c: users sharan babu anaconda3 lib site-packages (from jinja2&gt;=2.11.1-&gt;sweetviz) (1.1.1) Requirement already satisfied: six&gt;=1.5 in c: users sharan babu anaconda3 lib site-packages (from python-dateutil&gt;=2.6.1-&gt;pandas!=1.0.0,!=1.0.1,!=1.0.2,&gt;=0.25.3-&gt;sweetviz) (1.12.0) Requirement already satisfied: setuptools in c: users sharan babu anaconda3 lib site-packages (from kiwisolver&gt;=1.0.1-&gt;matplotlib&gt;=3.1.3-&gt;sweetviz) (41.0.1) Installing collected packages: tqdm, pandas, matplotlib, importlib-resources, jinja2, sweetviz Found existing installation: tqdm 4.32.1 Uninstalling tqdm-4.32.1: Successfully uninstalled tqdm-4.32.1 Found existing installation: pandas 0.24.2 Uninstalling pandas-0.24.2: Successfully uninstalled pandas-0.24.2 Found existing installation: matplotlib 3.1.0 Uninstalling matplotlib-3.1.0: Successfully uninstalled matplotlib-3.1.0 Found existing installation: Jinja2 2.10.1 Uninstalling Jinja2-2.10.1: Successfully uninstalled Jinja2-2.10.1 Successfully installed importlib-resources-2.0.0 jinja2-2.11.2 matplotlib-3.2.1 pandas-1.0.4 sweetviz-1.0a7 tqdm-4.46.1 . import numpy as np import pandas as pd import sweetviz . df = pd.read_csv(r&quot;C: Users Sharan Babu Desktop Data science original Refactored_Py_DS_ML_Bootcamp-master 11-Linear-Regression USA_housing.csv&quot;) . df.head() # In this dataset, price column is the target feature or dependent variable. . Avg. Area Income Avg. Area House Age Avg. Area Number of Rooms Avg. Area Number of Bedrooms Area Population Price Address . 0 79545.458574 | 5.682861 | 7.009188 | 4.09 | 23086.800503 | 1.059034e+06 | 208 Michael Ferry Apt. 674 nLaurabury, NE 3701... | . 1 79248.642455 | 6.002900 | 6.730821 | 3.09 | 40173.072174 | 1.505891e+06 | 188 Johnson Views Suite 079 nLake Kathleen, CA... | . 2 61287.067179 | 5.865890 | 8.512727 | 5.13 | 36882.159400 | 1.058988e+06 | 9127 Elizabeth Stravenue nDanieltown, WI 06482... | . 3 63345.240046 | 7.188236 | 5.586729 | 3.26 | 34310.242831 | 1.260617e+06 | USS Barnett nFPO AP 44820 | . 4 59982.197226 | 5.040555 | 7.839388 | 4.23 | 26354.109472 | 6.309435e+05 | USNS Raymond nFPO AE 09386 | . Analyzing a DataFrame . analysis = sweetviz.analyze([df,&quot;EDA&quot;], target_feat=&#39;Price&#39;) . :FEATURES DONE: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| [100%] 00:06 -&gt; (00:00 left) :PAIRWISE DONE: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| [100%] 00:00 -&gt; (00:00 left) . Creating Associations graph... DONE! . type(analysis) . sweetviz.dataframe_report.DataframeReport . analysis.show_html(&#39;EDA.html&#39;) . This is an amazing visualization library for your data as you instantly get various insights into your data which you could have done manually but would have taken a lot more time. For numerical features, you get point plot, histogram, number of value missing, number of distinct values, quartile values and more useful information like skewness of the column. For categorical features, along with the number of distinct and missing values, you Additionally, you also get the the &#39;Associations&#39; or pair-wise correlations between 2 variables which is helpful for determining feature importance. . You can also use this library to comapre two DataFrames,say, your Training set and Test set and infer some meaning from the comparison. . train = df[:3000] . test = df[3000:] . # Consider &#39;test&#39; to be the Test data. # The command to perform EDA comparison is: analysis = sweetviz.compare([train,&quot;Train&quot;],[test,&quot;Test&quot;], &quot;Price&quot;) # Price is the target variable common to both tables # Now you can view your results. analysis.show_html(&#39;EDA2.html&#39;) . :FEATURES DONE: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| [100%] 00:08 -&gt; (00:00 left) :PAIRWISE DONE: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| [100%] 00:00 -&gt; (00:00 left) . Creating Associations graph... DONE! . Now, you can see comparison between the Train and Test dataset differentiate by different colors for all paramters discussed above. Therefore, this is a handy module .",
            "url": "https://sharanbabu19.github.io/sharan19/2020/09/19/SweetViz-Automated-EDA.html",
            "relUrl": "/2020/09/19/SweetViz-Automated-EDA.html",
            "date": " ‚Ä¢ Sep 19, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi there, I am Sharan üëã . I am an aspiring Data Scientist pursuing an undergraduate degree in Computer Science. . ‚ö° Curiously exploring the depths of Deep Learning and SOTA models. | :man_technologist: I‚Äôm currently working on DeHazing Images. | üå± Currently learning Deep Learning. | :smile: Looking to collaborate on ML/Data Science Projects. | üí¨ Let‚Äôs talk about Hackathons, Machine Learning or Computer Vision. | :mailbox_with_mail: Email: sharanbabu2001@gmail.com | . Know more: . Github | LinkedIn | Portfolio | Medium | Resume | Youtube .",
          "url": "https://sharanbabu19.github.io/sharan19/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://sharanbabu19.github.io/sharan19/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}